import os
import re
import uuid
import torch
import requests
import gc
from qdrant_client import QdrantClient, models
from transformers import AutoTokenizer, AutoModelForCausalLM
from openai import OpenAI
from deepeval.models import DeepEvalBaseLLM
from deepeval.metrics import (
    FaithfulnessMetric,
    AnswerRelevancyMetric,
    ContextualRecallMetric,
    ContextualPrecisionMetric,
    ContextualRelevancyMetric,
)
from deepeval.test_case import LLMTestCase
from deepeval import evaluate
from deepeval.evaluate.configs import DisplayConfig, AsyncConfig

# ==========================================
# ã€ç³»çµ±å®‰å…¨è¨­å®šã€‘
# ==========================================
# å³ä½¿æœ‰ GPUï¼ŒCPU ä»è² è²¬è³‡æ–™è™•ç†ã€‚é™åˆ¶ä½¿ç”¨ 4 æ ¸å¿ƒï¼Œç¢ºä¿é›»è…¦ä¸å¡é “ã€‚
torch.set_num_threads(4)
os.environ["OMP_NUM_THREADS"] = "4"

# é¡¯ç¤ºç›®å‰çš„ PyTorch ç‹€æ…‹
print(f"PyTorch Version: {torch.__version__}")
print(f"CUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU Device: {torch.cuda.get_device_name(0)}")
    # é å…ˆæ¸…ç† GPU è¨˜æ†¶é«”
    torch.cuda.empty_cache()
    gc.collect()

# ==========================================

class LlamaCppModel(DeepEvalBaseLLM):
    def __init__(
        self,
        base_url="https://ws-02.wade0426.me/v1",
        model_name="gpt-4o"
    ):
        self.base_url = base_url
        self.model_name = model_name
        
    def load_model(self):
        return OpenAI(
            api_key="NoNeed",
            base_url=self.base_url,
            timeout=300.0
        )
    
    def generate(self, prompt: str) -> str:
        client = self.load_model()
        response = client.chat.completions.create(
            model=self.model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
        )
        return response.choices[0].message.content
    
    async def a_generate(self, prompt: str) -> str:
        return self.generate(prompt)
    
    def get_model_name(self):
        return f"Llama.cpp ({self.model_name})"


# ========== è¨­å®š ==========
QDRANT_URL = "http://localhost:6333"
COLLECTION_NAME = "day6_hw"
MODEL_PATH = r"c:\Users\berto\Desktop\lab\homework\test\day6\~\Models\Qwen3-Reranker-0.6B"
EMBEDDING_API_URL = "https://ws-04.wade0426.me/embed"
DATA_FILE = os.path.join(os.path.dirname(__file__), "hw_data", "qa_data.txt")

# ========== 1. åˆå§‹åŒ– Qdrant ==========
client = QdrantClient(url=QDRANT_URL)

# ========== 2. æ™ºæ…§è¼‰å…¥ Reranker æ¨¡å‹ ==========
print(f"\næ­£åœ¨è¼‰å…¥ Reranker æ¨¡å‹...")

# é è¨­åƒæ•¸
reranker_device = "cpu"
use_fp16 = False

# å˜—è©¦ä½¿ç”¨ GPU
if torch.cuda.is_available():
    try:
        print("å˜—è©¦å°‡æ¨¡å‹è¼‰å…¥ GPU (MX350) ä¸¦å•Ÿç”¨ FP16 å„ªåŒ–...")
        
        # é€™è£¡ä¸ç›´æ¥è¼‰å…¥ modelï¼Œå…ˆè¼‰å…¥ tokenizer
        reranker_tokenizer = AutoTokenizer.from_pretrained(
            MODEL_PATH,
            local_files_only=True,
            trust_remote_code=True,
            padding_side='left'
        )

        # å˜—è©¦è¼‰å…¥æ¨¡å‹åˆ° GPUï¼Œå¼·åˆ¶ä½¿ç”¨ FP16 (çœä¸€åŠè¨˜æ†¶é«”)
        reranker_model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            local_files_only=True,
            trust_remote_code=True,
            device_map="cuda",          # æŒ‡å®š GPU
            torch_dtype=torch.float16,  # ã€é—œéµã€‘ä½¿ç”¨ FP16 åŠç²¾åº¦
            low_cpu_mem_usage=True
        ).eval()
        
        reranker_device = "cuda"
        use_fp16 = True
        print("âœ… æˆåŠŸï¼æ¨¡å‹å·²åœ¨ GPU ä¸Šé‹è¡Œ (FP16 æ¨¡å¼)ã€‚")
        print("æ³¨æ„ï¼šMX350 åªæœ‰ 2GB VRAMï¼Œè«‹å‹¿é–‹å•Ÿå¤§é‡èƒŒæ™¯ç¨‹å¼ã€‚")

    except Exception as e:
        print(f"âŒ GPU è¼‰å…¥å¤±æ•— (å¯èƒ½æ˜¯è¨˜æ†¶é«”ä¸è¶³): {e}")
        print("ğŸ”„ æ­£åœ¨åˆ‡æ›å› CPU æ¨¡å¼ (ä¸ç”¨æ“”å¿ƒï¼Œé€™å¾ˆæ­£å¸¸)...")
        
        # æ¸…ç†å¤±æ•—çš„æ®˜ç•™è¨˜æ†¶é«”
        if 'reranker_model' in locals(): del reranker_model
        torch.cuda.empty_cache()
        gc.collect()
        
        # CPU Fallback
        reranker_model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            local_files_only=True,
            trust_remote_code=True,
            device_map="cpu",
            torch_dtype=torch.float32
        ).eval()
        reranker_device = "cpu"
else:
    print("æœªåµæ¸¬åˆ° GPUï¼Œç›´æ¥ä½¿ç”¨ CPU æ¨¡å¼ã€‚")
    # å¦‚æœæ²’ GPUï¼Œéœ€è¦è£œè¼‰å…¥ tokenizer
    if 'reranker_tokenizer' not in locals():
        reranker_tokenizer = AutoTokenizer.from_pretrained(
            MODEL_PATH,
            local_files_only=True,
            trust_remote_code=True,
            padding_side='left'
        )
    reranker_model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            local_files_only=True,
            trust_remote_code=True,
            device_map="cpu",
            torch_dtype=torch.float32
        ).eval()


# ç²å– token IDs
token_false_id = reranker_tokenizer.convert_tokens_to_ids("no")
token_true_id = reranker_tokenizer.convert_tokens_to_ids("yes")

# ã€é‡è¦ã€‘è¨­å®šæœ€å¤§é•·åº¦
# MX350 è¨˜æ†¶é«”å°ï¼Œè¨­å¤ªå¤§å¿…ç•¶æ©Ÿã€‚1024 å°å¤§å¤šæ•¸ QA è¶³å¤ ã€‚
# å¦‚æœé‚„æ˜¯çˆ†é¡¯å­˜ï¼Œè«‹å°‡æ­¤æ•¸å­—æ”¹ç‚º 512
max_reranker_length = 1024 
print(f"è¨­å®š Reranker æœ€å¤§é•·åº¦é™åˆ¶: {max_reranker_length} tokens")

prefix = "<|im_start|>system\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\".<|im_end|>\n<|im_start|>user\n"
suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"
prefix_tokens = reranker_tokenizer.encode(prefix, add_special_tokens=False)
suffix_tokens = reranker_tokenizer.encode(suffix, add_special_tokens=False)


# ========== è¼”åŠ©å‡½æ•¸ ==========

def get_embeddings(texts: list) -> list:
    data = {"texts": texts, "normalize": True, "batch_size": 32}
    try:
        response = requests.post(EMBEDDING_API_URL, json=data)
        if response.status_code == 200:
            return response.json()['embeddings']
        return []
    except Exception as e:
        print(f"Embedding Error: {e}")
        return []

def format_instruction(instruction, query, doc):
    if instruction is None: instruction = 'æ ¹æ“šæŸ¥è©¢æª¢ç´¢ç›¸é—œæ–‡ä»¶'
    return f"<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {doc}"

def process_inputs(pairs):
    inputs = reranker_tokenizer(
        pairs,
        padding=False,
        truncation='longest_first',
        return_attention_mask=False,
        max_length=max_reranker_length - len(prefix_tokens) - len(suffix_tokens)
    )
    for i, ele in enumerate(inputs['input_ids']):
        inputs['input_ids'][i] = prefix_tokens + ele + suffix_tokens
    
    inputs = reranker_tokenizer.pad(
        inputs,
        padding=True,
        return_tensors="pt",
        max_length=max_reranker_length
    )
    
    # å°‡è¼¸å…¥ç§»å‹•åˆ°èˆ‡æ¨¡å‹ç›¸åŒçš„è¨­å‚™ (GPU or CPU)
    for key in inputs:
        inputs[key] = inputs[key].to(reranker_model.device)
    
    return inputs

@torch.no_grad()
def compute_logits(inputs):
    # è¨ˆç®— logits
    batch_scores = reranker_model(**inputs).logits[:, -1, :]
    true_vector = batch_scores[:, token_true_id]
    false_vector = batch_scores[:, token_false_id]
    batch_scores = torch.stack([false_vector, true_vector], dim=1)
    batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)
    return batch_scores[:, 1].exp().tolist()

def rerank_documents(query, documents, task_instruction=None):
    if task_instruction is None: task_instruction = 'æ ¹æ“šæŸ¥è©¢æª¢ç´¢ç›¸é—œæ–‡ä»¶'
    texts = [doc[0] for doc in documents]
    pairs = [format_instruction(task_instruction, query, text) for text in texts]

    try:
        # MX350 æ‰¹æ¬¡è™•ç†èƒ½åŠ›æ¥µå¼±ï¼Œæˆ‘å€‘å¼·åˆ¶å°‡ batch_size è¨­ç‚º 1
        # é€™æœƒæ¯”ä¸€æ¬¡è™•ç†æ…¢ä¸€é»ï¼Œä½†èƒ½ä¿è­‰ä¸çˆ†é¡¯å­˜
        scores = []
        batch_size = 1 
        
        for i in range(0, len(pairs), batch_size):
            batch_pairs = pairs[i : i + batch_size]
            inputs = process_inputs(batch_pairs)
            batch_scores = compute_logits(inputs)
            scores.extend(batch_scores)
            
            # å¦‚æœåœ¨ GPU ä¸Šï¼Œè·‘å®Œä¸€ç­†å°±æ¸…ä¸€ä¸‹åƒåœ¾
            if reranker_device == "cuda":
                del inputs
                torch.cuda.empty_cache()

    except RuntimeError as e:
        if "out of memory" in str(e):
            print("âš ï¸ é¡¯å¡è¨˜æ†¶é«”ä¸è¶³ (OOM)ï¼Œç„¡æ³•åŸ·è¡Œé‡æ’ã€‚è·³éé‡æ’æ­¥é©Ÿã€‚")
            torch.cuda.empty_cache()
            return [(doc, 0.0) for doc in documents]
        else:
            print(f"Rerank Error: {e}")
            return [(doc, 0.0) for doc in documents]

    doc_scores = list(zip(documents, scores))
    doc_scores.sort(key=lambda x: x[1], reverse=True)
    return doc_scores

# ========== å…¶ä»–åŠŸèƒ½å‡½æ•¸ (åˆ‡åˆ†ã€æ··åˆæœç´¢ã€LLM) ==========

def split_text_qa_aware(text):
    # (ä¿æŒåŸæ¨£)
    chunks = []
    lines = text.split('\n')
    date_indices = [i for i, line in enumerate(lines) if '**ç™¼å¸ƒæ—¥æœŸ**' in line]
    if not date_indices: return [text]

    for idx, date_line_idx in enumerate(date_indices):
        question = lines[date_line_idx - 1].strip() if date_line_idx > 0 else ""
        date_match = re.search(r'(\d{4}/\d{2}/\d{2})', lines[date_line_idx])
        date_str = date_match.group(1) if date_match else ""
        end_boundary = date_indices[idx + 1] - 1 if idx + 1 < len(date_indices) else len(lines)
        
        content_lines = []
        source = ""
        for j in range(date_line_idx + 1, end_boundary):
            line = lines[j].strip()
            if not line: 
                content_lines.append("")
                continue
            if line.startswith('ä¾†æºï¼š'):
                source = line
                break
            content_lines.append(lines[j])
        
        while content_lines and not content_lines[-1].strip(): content_lines.pop()
        content = '\n'.join(content_lines).strip()
        
        if source: qa_unit = f"{question}\n**ç™¼å¸ƒæ—¥æœŸ**: {date_str}\n{content}\n{source}"
        else: qa_unit = f"{question}\n**ç™¼å¸ƒæ—¥æœŸ**: {date_str}\n{content}"
        chunks.append(qa_unit)
    return chunks

def hybrid_search_with_rerank(query: str, initial_limit: int = 10, final_limit: int = 3):
    embeddings = get_embeddings([query])
    if not embeddings: return []
    query_embedding = embeddings[0]

    try:
        response = client.query_points(
            collection_name=COLLECTION_NAME,
            prefetch=[
                models.Prefetch(query=models.Document(text=query, model="Qdrant/bm25"), using="sparse", limit=initial_limit),
                models.Prefetch(query=query_embedding, using="dense", limit=initial_limit),
            ],
            query=models.FusionQuery(fusion=models.Fusion.RRF),
            limit=initial_limit,
        )
    except Exception as e:
        print(f"Qdrant Search Error: {e}")
        return []

    candidate_docs = [(point.payload.get("text", ""), point.payload.get("source_file", "Unknown")) for point in response.points]
    if not candidate_docs: return []

    print(f"æ­£åœ¨é‡æ’ {len(candidate_docs)} å€‹æ–‡ä»¶ (ä½¿ç”¨ {reranker_device.upper()})...")
    reranked_results = rerank_documents(query, candidate_docs)
    
    top_results = reranked_results[:final_limit]
    print(f"Top {final_limit} åˆ†æ•¸: {[round(s, 3) for _, s in top_results]}")
    return top_results

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

llm = ChatOpenAI(
    model="/models/Qwen3-30B-A3B-Instruct-2507-FP8",
    temperature=0.2,
    base_url="https://ws-03.wade0426.me/v1",
    api_key="EMPTY",
)

def generate_answer_from_llm(query, context_parts):
    context_str = "\n\n".join(context_parts)
    prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„çŸ¥è­˜åº«åŠ©æ‰‹ã€‚è«‹æ ¹æ“šä»¥ä¸‹ã€åƒè€ƒè³‡è¨Šã€‘å›ç­”ã€ç”¨æˆ¶å•é¡Œã€‘ã€‚
### åƒè€ƒè³‡è¨Šï¼š
{context_str}
### ç”¨æˆ¶å•é¡Œï¼š
{query}
è«‹è¼¸å‡ºå›ç­”ï¼Œä¸¦åœ¨æœ€å¾Œä¸€è¡Œæ¨™è¨»ä¾†æº (SOURCE: æª”å)ï¼š"""
    try:
        messages = [HumanMessage(content=prompt)]
        response = llm.invoke(messages)
        return response.content.strip()
    except Exception as e:
        print(f"LLM Error: {e}")
        return "Error"

# ========== è©•ä¼°æµç¨‹ ==========
def evaluate_qa_with_deepeval():
    import openpyxl
    import traceback
    
    questions_file = "hw_data/day6_HW_questions.csv.xlsx"
    answers_file = "hw_data/questions_answer.csv.xlsx"
    
    print(f"\n========== DeepEval Evaluation ==========")
    
    # è¨­å®š DeepEval è¶…æ™‚æ™‚é–“ (é è¨­ 180s å¯èƒ½ä¸å¤ )
    os.environ["DEEPEVAL_PER_ATTEMPT_TIMEOUT_SECONDS_OVERRIDE"] = "1200"

    try:
        wb_q = openpyxl.load_workbook(questions_file)
        ws_q = wb_q.active
        wb_a = openpyxl.load_workbook(answers_file)
        ws_a = wb_a.active
        
        headers_q = [c.value for c in ws_q[1]]
        q_col_q = headers_q.index('questions') + 1
        headers_a = [c.value for c in ws_a[1]]
        a_col_a = headers_a.index('answer') + 1 if 'answer' in headers_a else 3

        if 'answer' in headers_q: ans_col_q = headers_q.index('answer') + 1
        else: ans_col_q = len(headers_q) + 1; ws_q.cell(1, ans_col_q, 'answer')

        test_cases = []
        row_indices = []
        generated_answers = []
        
        # å®šç¾©éœ€è¦è©•ä¼°çš„æŒ‡æ¨™èˆ‡å°æ‡‰æ¬„ä½
        metric_cols = {
            "Faithfulness": "Faithfulness",
            "Answer Relevancy": "Answer_Relevancy",
            "Contextual Recall": "Contextual_Recall",
            "Contextual Precision": "Contextual_Precision",
            "Contextual Relevancy": "Contextual_Relevancy"
        }
        
        # ç¢ºä¿ Excel ä¸­æœ‰é€™äº›æ¬„ä½ (å¦‚æœæ²’æœ‰å‰‡æ–°å¢)
        col_mapping = {}
        for m_name, col_name in metric_cols.items():
            if col_name in headers_q:
                col_mapping[m_name] = headers_q.index(col_name) + 1
            else:
                new_col = len(headers_q) + 1
                ws_q.cell(row=1, column=new_col, value=col_name)
                headers_q.append(col_name)
                col_mapping[m_name] = new_col

        # è™•ç†ç¬¬ 3-5 ç­†è³‡æ–™ (Excel Row 4, 5, 6)
        target_rows = range(4, 7)
        print(f"å°‡è™•ç† Row {min(target_rows)} åˆ° {max(target_rows)} çš„è³‡æ–™...")

        for row_idx in target_rows: 
            question = ws_q.cell(row=row_idx, column=q_col_q).value
            if not question: continue
            
            ground_truth = ws_a.cell(row=row_idx, column=a_col_a).value or "ç„¡æ¨™æº–ç­”æ¡ˆ"
            print(f"\n[Row {row_idx}] Processing: {question}")
            
            results = hybrid_search_with_rerank(str(question).strip(), initial_limit=10, final_limit=4)
            retrieval_context = [txt for (txt, _), _ in results]
            context_parts = [f"[ä¾†æº: {src}]\n{txt}" for (txt, src), _ in results]
            
            raw_answer = generate_answer_from_llm(question, context_parts)
            actual_output = raw_answer.split("SOURCE:")[0].strip() if "SOURCE:" in raw_answer else raw_answer
            print(f"  > Generated: {actual_output[:20]}...")
            
            # å…ˆå¡«å…¥ç”Ÿæˆçš„å›ç­” (è¦†å¯«æˆ–æ–°å¢)
            ws_q.cell(row=row_idx, column=ans_col_q, value=actual_output)

            test_cases.append(LLMTestCase(
                input=str(question),
                actual_output=actual_output,
                expected_output=str(ground_truth),
                retrieval_context=retrieval_context
            ))
            row_indices.append(row_idx)
            generated_answers.append(actual_output)

        if not test_cases: return

        print("åˆå§‹åŒ–è©•ä¼°æ¨¡å‹...")
        # è©•ä¼°æ¨¡å‹é‚„æ˜¯æœƒæ¯”è¼ƒåƒè³‡æºï¼ŒDeepEval å…§éƒ¨ç„¡æ³•æ§åˆ¶é€™éº¼ç´°
        # ä½†å› ç‚ºæˆ‘å€‘å‰é¢å·²ç¶“æŠŠ heavy loading çš„ Reranker è™•ç†å®Œäº†ï¼Œé€™è£¡æ‡‰è©²é‚„å¥½
        judge_llm = LlamaCppModel()
        
        metrics = [
            FaithfulnessMetric(threshold=0.7, model=judge_llm, include_reason=False),
            AnswerRelevancyMetric(threshold=0.7, model=judge_llm, include_reason=False),
            ContextualRecallMetric(threshold=0.7, model=judge_llm, include_reason=False),
            ContextualPrecisionMetric(threshold=0.7, model=judge_llm, include_reason=False),
            ContextualRelevancyMetric(threshold=0.7, model=judge_llm, include_reason=False),
        ]
        
        print(f"é–‹å§‹è©•ä¼° {len(test_cases)} å€‹æ¡ˆä¾‹...")
        test_results = evaluate(
            test_cases, 
            metrics,
            display_config=DisplayConfig(print_results=False, show_indicator=True),
            async_config=AsyncConfig(run_async=False)
        )
        print("è©•ä¼°å®Œæˆï¼Œæ­£åœ¨å¯«å…¥çµæœ...")

        # å°‡è©•ä¼°çµæœå¯«å› Excel
        try:
            for i, test_case_result in enumerate(test_results.test_results):
                row_idx = row_indices[i]
                for metric_data in test_case_result.metrics_data:
                    metric_name = metric_data.name
                    if metric_name in col_mapping:
                        col_idx = col_mapping[metric_name]
                        score = metric_data.score
                        ws_q.cell(row=row_idx, column=col_idx, value=score)
                        print(f"  Row {row_idx} - {metric_name}: {score:.4f}")
        except AttributeError as e:
            print(f"å¯«å…¥è©•åˆ†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            print("test_results çµæ§‹:", dir(test_results))

        # å„²å­˜çµæœ
        output_file = "hw_data/day6_HW_questions_result.xlsx"
        wb_q.save(output_file)
        print(f"çµæœå·²å„²å­˜è‡³: {output_file}")

    except Exception as e:
        print(f"Error: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    evaluate_qa_with_deepeval()