import os
import json
from typing import TypedDict, Literal, List, Annotated
import operator
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.graph import StateGraph, END
from pydantic import BaseModel, Field
from langchain_google_vertexai import ChatVertexAI
# å°å…¥è‡ªå®šç¾©å·¥å…·
from search_searxng import search_searxng
from vlm_read_website import vlm_read_website

# --- é…ç½® ---
CACHE_FILE = "hw4_cache.json"

llm = ChatOpenAI(
    base_url="https://ws-05.huannago.com/v1",
    api_key="EMPTY",
    model="Qwen3-VL-8B-Instruct-BF16.gguf",
    temperature=0.7
)
# llm = ChatVertexAI(
#     model="gemini-2.5-pro",
#     project="gen-lang-client-0342191491",  # <--- é—œéµï¼é€™è£¡å¡«å°ï¼ŒéŒ¢å°±å¾æŠµå…é¡å‡º
#     location="us-central1",     # å»ºè­°é¸ us-central1 è³‡æºæœ€è±å¯Œ
#     temperature=0.7
# )
# --- ç‹€æ…‹å®šç¾© ---
class State(TypedDict):
    question: str
    answer: str
    source: str  # CACHE / LLM
    search_results: List[dict]  # SearXNG æœå°‹çµæœ
    vlm_content: str  # VLM è®€å–çš„å…§å®¹
    loop_count: int   # é˜²æ­¢ç„¡é™è¿´åœˆ
    reasoning: str    # è¦åŠƒå™¨çš„æ€è€ƒéç¨‹
    current_query: str
    decision: Literal["sufficient", "insufficient"] # æ±ºç­–çµæœ

# --- è¼”åŠ©å‡½æ•¸ ---
def load_cache():
    if not os.path.exists(CACHE_FILE):
        return {}
    try:
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        return {}

def save_cache(new_data):
    current_data = load_cache()
    current_data.update(new_data)
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(current_data, f, ensure_ascii=False, indent=4)

# --- ç¯€é» (Nodes) ---

def check_cache_node(state: State):
    """1. æª¢æŸ¥å¿«å–"""
    print(f"\n[ç³»çµ±] æ­£åœ¨æª¢æŸ¥å¿«å–ï¼š{state['question']}")
    cache_data = load_cache()
    
    if state['question'] in cache_data:
        print("--- å‘½ä¸­å¿«å– (Cache Hit) ---")
        return {
            "answer": cache_data[state['question']],
            "source": "CACHE",
            "loop_count": 0
        }
    else:
        print("--- æœªå‘½ä¸­å¿«å– (Cache Miss) ---")
        return {
            "source": "LLM", 
            "loop_count": 0,
            "vlm_content": "",
            "search_results": []
        }

class PlannerDecision(BaseModel):
    reasoning: str = Field(description="åˆ†æç›®å‰è³‡è¨Šæ˜¯å¦è¶³å¤ å›ç­”å•é¡Œ")
    decision: Literal["sufficient", "insufficient"] = Field(description="æ±ºå®šæ˜¯å¦å›ç­”æˆ–ç¹¼çºŒæœå°‹")

def planner_node(state: State):
    """2. è¦åŠƒå™¨ / æ±ºç­–ç¯€é»"""
    current_loop = state.get("loop_count", 0)
    print(f"\n[ç³»çµ±] è¦åŠƒå™¨æ­£åœ¨æ€è€ƒ... (ç›®å‰è¿´åœˆæ¬¡æ•¸: {current_loop})")
    
    # æª¢æŸ¥è¿´åœˆé™åˆ¶
    if current_loop >= 3:
        print("--- é”åˆ°è¿´åœˆé™åˆ¶ï¼Œå¼·åˆ¶å›ç­” ---")
        return {"decision": "sufficient", "reasoning": "é”åˆ°è¿´åœˆé™åˆ¶"}

    prompt = f"""
    ä½¿ç”¨è€…å•é¡Œ: {state['question']}
    
    ç›®å‰æ”¶é›†çš„è³‡è¨Š:
    {state.get('vlm_content', 'ç„¡')}
    
    æœå°‹çµæœ:
    {str(state.get('search_results', []))[:500]}...
    
    è«‹åˆ¤æ–·ç›®å‰æ”¶é›†çš„è³‡è¨Šæ˜¯å¦è¶³ä»¥æº–ç¢ºå›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
    å¦‚æœæ˜¯ï¼Œè¼¸å‡º 'sufficient'ã€‚
    å¦‚æœå¦ï¼Œè¼¸å‡º 'insufficient' ä»¥åŸ·è¡Œæ›´å¤šæœå°‹/ç ”ç©¶ã€‚
    """
    
    structured_llm = llm.with_structured_output(PlannerDecision)
    result = structured_llm.invoke(prompt)
    
    print(f"--- æ±ºç­–: {result.decision} ({result.reasoning}) ---")
    return {"decision": result.decision, "reasoning": result.reasoning}

def query_gen_node(state: State):
    """3. ç”Ÿæˆæœå°‹é—œéµå­—"""
    print("\n[ç³»çµ±] æ­£åœ¨ç”Ÿæˆæœå°‹é—œéµå­—...")
    
    try:
        prompt = f"æ ¹æ“šå•é¡Œ '{state['question']}'ï¼Œç”Ÿæˆä¸€å€‹å…·é«”çš„ Google æœå°‹é—œéµå­—ä»¥å°‹æ‰¾ç­”æ¡ˆã€‚åƒ…è¼¸å‡ºé—œéµå­—æ–‡å­—ã€‚"
        # åŠ å…¥è¶…æ™‚æ§åˆ¶ (å¦‚æœæ¨¡å‹æ”¯æ´ timeout åƒæ•¸ï¼Œå¦å‰‡æ¨™æº– invoke å¯èƒ½ä¸æ”¯æ´ï¼Œé€™è£¡åŠ  try-except æ˜¯æ ¸å¿ƒ)
        response = llm.invoke(prompt)
        query = response.content.strip().replace('"', '')
        print(f"--- é—œéµå­—: {query} ---")
        return {"current_query": query}
    except Exception as e:
        print(f"âŒ ç”Ÿæˆé—œéµå­—å¤±æ•—: {e}")
        return {"current_query": state['question']} 

def search_tool_node(state: State):
    """4. åŸ·è¡Œæœå°‹"""
    query = state.get("current_query", state["question"])
    print(f"\n[ç³»çµ±] æ­£åœ¨æœå°‹: {query}")
    
    results = search_searxng(query, limit=3)
    return {"search_results": results}

def vlm_process_node(state: State):
    """5. ä½¿ç”¨ VLM è®€å–ç¶²ç«™"""
    results = state.get("search_results", [])
    current_loop = state.get("loop_count", 0)
    
    if not results:
        print("--- æ²’æœ‰æœå°‹çµæœå¯ä¾›è®€å– ---")
        return {
            "vlm_content": "æœªæ‰¾åˆ°æœå°‹çµæœã€‚",
            "loop_count": current_loop + 1
        }
    
    # é¸æ“‡ç¬¬ä¸€å€‹æœ‰æ•ˆçš„ URL
    target_url = results[0].get("url")
    title = results[0].get("title", "ç„¡æ¨™é¡Œ")
    
    if not target_url:
        return {
            "vlm_content": "æœå°‹çµæœä¸­æ²’æœ‰æœ‰æ•ˆçš„ URLã€‚",
            "loop_count": current_loop + 1
        }
        
    print(f"\n[ç³»çµ±] VLM æ­£åœ¨è®€å–: {target_url}")
    content = vlm_read_website(target_url, title)
    
    return {
        "vlm_content": content,
        "loop_count": current_loop + 1
    }

def final_answer_node(state: State):
    """6. ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ"""
    print("\n[ç³»çµ±] æ­£åœ¨ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ...")
    
    prompt = f"""
    å•é¡Œ: {state['question']}
    
    å·²é©—è­‰è³‡è¨Š:
    {state.get('vlm_content', 'ç„¡å…§å®¹')}
    
    æœå°‹ä¸Šä¸‹æ–‡:
    {state.get('search_results', [])}
    
    è«‹æä¾›ä¸€å€‹å…¨é¢ä¸”è¦ªåˆ‡çš„å›ç­”çµ¦ä½¿ç”¨è€…ã€‚
    """
    
    response = llm.invoke(prompt)
    answer = response.content
    
    # æ›´æ–°å¿«å–
    save_cache({state['question']: answer})
    
    return {"answer": answer}

# --- é‚Š (Edges) ---

def route_check_cache(state: State):
    if state.get("source") == "CACHE":
        return "end"
    return "planner"

# --- åœ–å»ºæ§‹ (Graph Construction) ---
workflow = StateGraph(State) 

workflow.add_node("check_cache", check_cache_node)
workflow.add_node("planner", planner_node)
workflow.add_node("query_gen", query_gen_node)
workflow.add_node("search_tool", search_tool_node)
workflow.add_node("vlm_process", vlm_process_node)
workflow.add_node("final_answer", final_answer_node)

workflow.set_entry_point("check_cache")

workflow.add_conditional_edges(
    "check_cache",
    route_check_cache,
    {
        "end": END,
        "planner": "planner"
    }
)

workflow.add_conditional_edges(
    "planner",
    lambda x: x['decision'],
    {
        "sufficient": "final_answer",
        "insufficient": "query_gen"
    }
)

workflow.add_edge("query_gen", "search_tool")
workflow.add_edge("search_tool", "vlm_process")
workflow.add_edge("vlm_process", "planner") # è¿´åœˆæª¢æŸ¥æ˜¯å¦è¶³å¤ 

workflow.add_edge("final_answer", END)

app = workflow.compile()
print(app.get_graph().draw_ascii())
# --- åŸ·è¡Œ ---
if __name__ == "__main__":
    user_input = input("æˆ‘æ˜¯å…¨èƒ½æŸ¥è­‰ AI åŠ©æ‰‹ï¼Œè«‹å•æœ‰ä»€éº¼æƒ³çŸ¥é“çš„å—ï¼Ÿ").strip()
    if not user_input:
        user_input = "æœ€è¿‘èª°çˆ¬äº†101å¤§æ¨“"

    print("\n" + "="*50)
    print("ğŸ” é–‹å§‹æŸ¥è­‰æµç¨‹")
    print("="*50)
    
    # ä½¿ç”¨ invoke å–ä»£ stream
    result = app.invoke(
        {"question": user_input, "loop_count": 0},
        config={"recursion_limit": 50}
    )
    
    # è¼¸å‡ºæœ€çµ‚çµæœ
    print("\n" + "="*50)
    print("ğŸ“ æŸ¥è­‰çµæœ")
    print("="*50)
    print(f"ä¾†æº: {result.get('source', 'LLM')}")
    print(f"è¿´åœˆæ¬¡æ•¸: {result.get('loop_count', 0)}")
    
    print("\n[å›ç­”]")
    print(result.get("answer", "ç„¡æ³•ç”Ÿæˆç­”æ¡ˆ"))
    
    # é¡¯ç¤ºåƒè€ƒä¾†æº
    results = result.get('search_results', [])
    if results:
        print("\n[åƒè€ƒä¾†æº]")
        for idx, res in enumerate(results, 1):
            title = res.get('title', 'ç„¡æ¨™é¡Œ')
            url = res.get('url', 'ç„¡é€£çµ')
            print(f"{idx}. {title} ({url})")